{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca9def5",
   "metadata": {},
   "source": [
    "# Fraudulent Transaction Detection System\n",
    "\n",
    "## üìä Project Overview\n",
    "\n",
    "This notebook implements a comprehensive machine learning solution for detecting fraudulent financial transactions. The system addresses the critical challenge of highly imbalanced datasets in fraud detection, where legitimate transactions vastly outnumber fraudulent ones.\n",
    "\n",
    "### Key Objectives:\n",
    "- Minimize false negatives while maintaining reasonable precision\n",
    "- Handle extreme class imbalance (fraudulent transactions < 0.2%)\n",
    "- Build efficient models for real-time transaction processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8719989b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fdda7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Machine Learning - Preprocessing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Machine Learning - Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "except ImportError:\n",
    "    print(\"LightGBM not installed. Install with: pip install lightgbm\")\n",
    "\n",
    "# Machine Learning - Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, precision_recall_curve, auc\n",
    ")\n",
    "\n",
    "# Imbalanced Learning\n",
    "try:\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "except ImportError:\n",
    "    print(\"Imbalanced-learn not installed. Install with: pip install imbalanced-learn\")\n",
    "\n",
    "# Check GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"PyTorch available: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed (optional for deep learning)\")\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27201f87",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8363bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Replace 'fraud_data.csv' with your actual dataset path\n",
    "df = pd.read_csv('fraud_data.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"First few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ec8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"Dataset Info:\")\n",
    "print(\"=\"*50)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\"*50)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5ce2a",
   "metadata": {},
   "source": [
    "## 3. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(\"=\"*50)\n",
    "missing_data = df.isnull().sum()\n",
    "missing_percent = (missing_data / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing_data,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "if missing_df['Missing Count'].sum() == 0:\n",
    "    print(\"\\n‚úÖ No missing values found!\")\n",
    "\n",
    "# Check for NaN values\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"NaN Value Check:\")\n",
    "print(\"=\"*50)\n",
    "nan_count = np.isnan(df.select_dtypes(include=[np.number]).values).sum()\n",
    "print(f\"Total NaN values in numerical columns: {nan_count}\")\n",
    "if nan_count == 0:\n",
    "    print(\"‚úÖ No NaN values present!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0418a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "print(\"Duplicate Rows Check:\")\n",
    "print(\"=\"*50)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "print(f\"Percentage: {(duplicates/len(df))*100:.2f}%\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(\"\\nRemoving duplicates...\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"‚úÖ Duplicates removed. New shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19608fef",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 4.1 Target Variable Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c424bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the target column is named 'isFraud' or 'fraud'\n",
    "# Adjust the column name based on your dataset\n",
    "target_col = 'isFraud' if 'isFraud' in df.columns else 'fraud'\n",
    "\n",
    "# Class distribution\n",
    "print(\"Class Distribution:\")\n",
    "print(\"=\"*50)\n",
    "class_dist = df[target_col].value_counts()\n",
    "print(class_dist)\n",
    "print(f\"\\nFraud Percentage: {(class_dist[1]/len(df))*100:.4f}%\")\n",
    "print(f\"Legitimate Percentage: {(class_dist[0]/len(df))*100:.4f}%\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "class_dist.plot(kind='bar', ax=axes[0], color=['green', 'red'])\n",
    "axes[0].set_title('Transaction Count by Class', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class (0: Legitimate, 1: Fraud)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Legitimate', 'Fraud'], rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(class_dist, labels=['Legitimate', 'Fraud'], autopct='%1.2f%%',\n",
    "            colors=['green', 'red'], startangle=90)\n",
    "axes[1].set_title('Class Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Note: Highly imbalanced dataset! Special handling required.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1df0d9",
   "metadata": {},
   "source": [
    "### 4.2 Transaction Type Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f2197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction type distribution (if 'type' column exists)\n",
    "if 'type' in df.columns:\n",
    "    print(\"Transaction Type Distribution:\")\n",
    "    print(\"=\"*50)\n",
    "    print(df['type'].value_counts())\n",
    "    \n",
    "    # Transaction type vs fraud\n",
    "    fig = px.histogram(df, x='type', color=target_col,\n",
    "                       title='Transaction Type Distribution by Fraud Status',\n",
    "                       labels={'type': 'Transaction Type', target_col: 'Fraud Status'},\n",
    "                       barmode='group')\n",
    "    fig.show()\n",
    "    \n",
    "    # Fraud rate by transaction type\n",
    "    fraud_by_type = df.groupby('type')[target_col].agg(['sum', 'count', 'mean'])\n",
    "    fraud_by_type.columns = ['Fraud Count', 'Total Count', 'Fraud Rate']\n",
    "    fraud_by_type['Fraud Rate'] = fraud_by_type['Fraud Rate'] * 100\n",
    "    print(\"\\nFraud Rate by Transaction Type:\")\n",
    "    print(fraud_by_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc7046",
   "metadata": {},
   "source": [
    "### 4.3 Amount Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af209afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount analysis (if 'amount' column exists)\n",
    "if 'amount' in df.columns:\n",
    "    print(\"Amount Statistics by Fraud Status:\")\n",
    "    print(\"=\"*50)\n",
    "    print(df.groupby(target_col)['amount'].describe())\n",
    "    \n",
    "    # Distribution plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Box plot\n",
    "    df.boxplot(column='amount', by=target_col, ax=axes[0])\n",
    "    axes[0].set_title('Amount Distribution by Fraud Status')\n",
    "    axes[0].set_xlabel('Fraud Status (0: Legitimate, 1: Fraud)')\n",
    "    axes[0].set_ylabel('Amount')\n",
    "    \n",
    "    # Histogram\n",
    "    for fraud_status in [0, 1]:\n",
    "        data = df[df[target_col] == fraud_status]['amount']\n",
    "        label = 'Fraud' if fraud_status == 1 else 'Legitimate'\n",
    "        axes[1].hist(np.log1p(data), bins=50, alpha=0.6, label=label)\n",
    "    axes[1].set_title('Log(Amount) Distribution')\n",
    "    axes[1].set_xlabel('Log(Amount + 1)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b1f13",
   "metadata": {},
   "source": [
    "### 4.4 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7481ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Calculate correlation\n",
    "correlation_matrix = df[numerical_cols].corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm',\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation with target\n",
    "if target_col in numerical_cols:\n",
    "    print(\"\\nCorrelation with Target Variable:\")\n",
    "    print(\"=\"*50)\n",
    "    target_corr = correlation_matrix[target_col].sort_values(ascending=False)\n",
    "    print(target_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5797b504",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58efa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for feature engineering\n",
    "df_fe = df.copy()\n",
    "\n",
    "# Apply transformations based on the app.py logic\n",
    "if 'newbalanceOrig' in df_fe.columns:\n",
    "    df_fe['newbalanceOrig_transformed'] = np.log1p(df_fe['newbalanceOrig'])\n",
    "    df_fe['newbalanceOrig_flag'] = (df_fe['newbalanceOrig_transformed'] < 5).astype(int)\n",
    "\n",
    "if 'oldbalanceOrg' in df_fe.columns:\n",
    "    df_fe['oldbalanceOrg_transformed'] = np.log1p(df_fe['oldbalanceOrg'])\n",
    "\n",
    "if 'amount' in df_fe.columns:\n",
    "    df_fe['amount_transformed'] = np.log1p(df_fe['amount'])\n",
    "\n",
    "if 'newbalanceDest' in df_fe.columns:\n",
    "    df_fe['newbalanceDest_transformed'] = np.log1p(df_fe['newbalanceDest'])\n",
    "\n",
    "# Encode transaction type\n",
    "if 'type' in df_fe.columns:\n",
    "    le = LabelEncoder()\n",
    "    df_fe['type_encoded'] = le.fit_transform(df_fe['type'])\n",
    "    df_fe['type_1_flag'] = (df_fe['type_encoded'] == 1).astype(int)\n",
    "\n",
    "# Additional features\n",
    "if 'oldbalanceOrg' in df_fe.columns and 'newbalanceOrig' in df_fe.columns:\n",
    "    df_fe['balance_change_orig'] = df_fe['oldbalanceOrg'] - df_fe['newbalanceOrig']\n",
    "\n",
    "if 'oldbalanceDest' in df_fe.columns and 'newbalanceDest' in df_fe.columns:\n",
    "    df_fe['balance_change_dest'] = df_fe['newbalanceDest'] - df_fe['oldbalanceDest']\n",
    "\n",
    "print(\"Feature Engineering Complete!\")\n",
    "print(f\"New shape: {df_fe.shape}\")\n",
    "print(f\"\\nNew features created:\")\n",
    "new_features = set(df_fe.columns) - set(df.columns)\n",
    "for feature in new_features:\n",
    "    print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62595f97",
   "metadata": {},
   "source": [
    "## 6. Data Preparation for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a37e99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling based on app.py\n",
    "feature_columns = ['step', 'type_encoded', 'amount', 'oldbalanceOrg', 'newbalanceOrig',\n",
    "                   'newbalanceDest', 'newbalanceOrig_transformed', 'newbalanceOrig_flag',\n",
    "                   'oldbalanceOrg_transformed', 'amount_transformed', \n",
    "                   'newbalanceDest_transformed', 'type_1_flag']\n",
    "\n",
    "# Filter to available columns\n",
    "available_features = [col for col in feature_columns if col in df_fe.columns]\n",
    "\n",
    "print(f\"Using {len(available_features)} features for modeling:\")\n",
    "for feature in available_features:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = df_fe[available_features]\n",
    "y = df_fe[target_col]\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Check dimensions\n",
    "assert X.shape[0] == y.shape[0], \"Mismatch between features and target!\"\n",
    "print(\"\\n‚úÖ Dimension check passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e05a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train-Test Split:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set fraud rate: {y_train.mean()*100:.4f}%\")\n",
    "print(f\"Test set fraud rate: {y_test.mean()*100:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba360b62",
   "metadata": {},
   "source": [
    "## 7. Handle Class Imbalance\n",
    "\n",
    "### 7.1 SMOTE (Synthetic Minority Oversampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE\n",
    "try:\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(\"Before SMOTE:\")\n",
    "    print(f\"  Class 0: {(y_train == 0).sum()}\")\n",
    "    print(f\"  Class 1: {(y_train == 1).sum()}\")\n",
    "    print(f\"  Fraud rate: {y_train.mean()*100:.4f}%\")\n",
    "    \n",
    "    print(\"\\nAfter SMOTE:\")\n",
    "    print(f\"  Class 0: {(y_train_smote == 0).sum()}\")\n",
    "    print(f\"  Class 1: {(y_train_smote == 1).sum()}\")\n",
    "    print(f\"  Fraud rate: {y_train_smote.mean()*100:.4f}%\")\n",
    "    \n",
    "    print(\"\\n‚úÖ SMOTE applied successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error applying SMOTE: {e}\")\n",
    "    print(\"Proceeding without SMOTE...\")\n",
    "    X_train_smote, y_train_smote = X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235444a",
   "metadata": {},
   "source": [
    "## 8. Model Training\n",
    "\n",
    "### 8.1 Baseline Model - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a24e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "lr_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "y_pred_proba_lr = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba_lr):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_lr, target_names=['Legitimate', 'Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1dcc16",
   "metadata": {},
   "source": [
    "### 8.2 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042c56b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    class_weight='balanced',\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba_rf):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=['Legitimate', 'Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf19090d",
   "metadata": {},
   "source": [
    "### 8.3 XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b79924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "\n",
    "# Calculate scale_pos_weight for imbalanced data\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nXGBoost Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba_xgb):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=['Legitimate', 'Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e5edc",
   "metadata": {},
   "source": [
    "### 8.4 Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b944a26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "print(\"Training Gradient Boosting...\")\n",
    "gb_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "gb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predictions\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "y_pred_proba_gb = gb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nGradient Boosting Results:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_gb):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_gb):.4f}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_gb):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_gb):.4f}\")\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba_gb):.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_gb, target_names=['Legitimate', 'Fraud']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654c028",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d8c912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "models = ['Logistic Regression', 'Random Forest', 'XGBoost', 'Gradient Boosting']\n",
    "predictions = [y_pred_lr, y_pred_rf, y_pred_xgb, y_pred_gb]\n",
    "predictions_proba = [y_pred_proba_lr, y_pred_proba_rf, y_pred_proba_xgb, y_pred_proba_gb]\n",
    "\n",
    "results = []\n",
    "for model_name, y_pred, y_pred_proba in zip(models, predictions, predictions_proba):\n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1-Score': f1_score(y_test, y_pred),\n",
    "        'ROC-AUC': roc_auc_score(y_test, y_pred_proba)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig = go.Figure()\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "\n",
    "for model in models:\n",
    "    model_data = results_df[results_df['Model'] == model]\n",
    "    fig.add_trace(go.Bar(\n",
    "        name=model,\n",
    "        x=metrics,\n",
    "        y=[model_data[metric].values[0] for metric in metrics]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison',\n",
    "    xaxis_title='Metrics',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba389f",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3611e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (model_name, y_pred) in enumerate(zip(models, predictions)):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                xticklabels=['Legitimate', 'Fraud'],\n",
    "                yticklabels=['Legitimate', 'Fraud'])\n",
    "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "    \n",
    "    # Add performance metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    axes[idx].text(0.5, -0.15, \n",
    "                   f'TP: {tp} | FP: {fp} | TN: {tn} | FN: {fn}',\n",
    "                   transform=axes[idx].transAxes,\n",
    "                   ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5b659",
   "metadata": {},
   "source": [
    "## 11. ROC Curve Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81e851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, y_pred_proba in zip(models, predictions_proba):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.4f})', linewidth=2)\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78219d",
   "metadata": {},
   "source": [
    "## 12. Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f02c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Precision-Recall curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for model_name, y_pred_proba in zip(models, predictions_proba):\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    plt.plot(recall, precision, label=f'{model_name} (AUC = {pr_auc:.4f})', linewidth=2)\n",
    "\n",
    "plt.xlabel('Recall', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curves - Model Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower left', fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa2eee",
   "metadata": {},
   "source": [
    "## 13. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a4fbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for tree-based models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "tree_models = [\n",
    "    ('Random Forest', rf_model),\n",
    "    ('XGBoost', xgb_model),\n",
    "    ('Gradient Boosting', gb_model)\n",
    "]\n",
    "\n",
    "for idx, (name, model) in enumerate(tree_models):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': available_features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    axes[idx].barh(feature_importance['feature'][:10], feature_importance['importance'][:10])\n",
    "    axes[idx].set_xlabel('Importance')\n",
    "    axes[idx].set_title(f'{name}\\nTop 10 Features', fontweight='bold')\n",
    "    axes[idx].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615a913f",
   "metadata": {},
   "source": [
    "## 14. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca0aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best model based on F1-Score (good balance for fraud detection)\n",
    "best_model_idx = results_df['F1-Score'].idxmax()\n",
    "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"F1-Score: {results_df.loc[best_model_idx, 'F1-Score']:.4f}\")\n",
    "\n",
    "# Map to actual model object\n",
    "model_map = {\n",
    "    'Logistic Regression': lr_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'XGBoost': xgb_model,\n",
    "    'Gradient Boosting': gb_model\n",
    "}\n",
    "\n",
    "best_model = model_map[best_model_name]\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(best_model, 'fraud_detection_model.pkl')\n",
    "print(f\"\\n‚úÖ Model saved as 'fraud_detection_model.pkl'\")\n",
    "\n",
    "# Save feature names for future use\n",
    "joblib.dump(available_features, 'model_features.pkl')\n",
    "print(\"‚úÖ Feature names saved as 'model_features.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcefecbb",
   "metadata": {},
   "source": [
    "## 15. Model Testing & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffac096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dimension check\n",
    "print(\"Testing Model Input Dimensions:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if hasattr(best_model, 'n_features_in_'):\n",
    "    model_features = best_model.n_features_in_\n",
    "    input_features = X_test.shape[1]\n",
    "    \n",
    "    print(f\"Model expects: {model_features} features\")\n",
    "    print(f\"Input has: {input_features} features\")\n",
    "    \n",
    "    if input_features == model_features:\n",
    "        print(\"‚úÖ Dimension check PASSED!\")\n",
    "    else:\n",
    "        print(\"‚ùå Dimension mismatch!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Model doesn't have n_features_in_ attribute\")\n",
    "\n",
    "# Test for NaN values\n",
    "print(\"\\nTesting for NaN Values:\")\n",
    "print(\"=\"*50)\n",
    "nan_count = np.isnan(X_test.values).sum()\n",
    "if nan_count == 0:\n",
    "    print(\"‚úÖ No NaN values in test data!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Found {nan_count} NaN values in test data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534bcee",
   "metadata": {},
   "source": [
    "## 16. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d2aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on sample data\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "sample_size = 10\n",
    "sample_indices = np.random.choice(X_test.index, sample_size, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    sample = X_test.loc[idx:idx]\n",
    "    actual = y_test.loc[idx]\n",
    "    prediction = best_model.predict(sample)[0]\n",
    "    probability = best_model.predict_proba(sample)[0][1]\n",
    "    \n",
    "    status = \"‚úÖ CORRECT\" if prediction == actual else \"‚ùå WRONG\"\n",
    "    actual_label = \"FRAUD\" if actual == 1 else \"LEGITIMATE\"\n",
    "    pred_label = \"FRAUD\" if prediction == 1 else \"LEGITIMATE\"\n",
    "    \n",
    "    print(f\"Sample {idx}:\")\n",
    "    print(f\"  Actual: {actual_label} | Predicted: {pred_label} | Probability: {probability:.4f}\")\n",
    "    print(f\"  {status}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13142b",
   "metadata": {},
   "source": [
    "## 17. GPU Availability Check (Optional for Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86565b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability for future deep learning models\n",
    "print(\"GPU Availability Check:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(\"‚úÖ GPU is available!\")\n",
    "        print(f\"   Device name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"   Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è GPU not available. Using CPU.\")\n",
    "        print(\"   For deep learning models, consider using GPU for faster training.\")\n",
    "except ImportError:\n",
    "    print(\"‚ÑπÔ∏è PyTorch not installed.\")\n",
    "    print(\"   Install with: pip install torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0d54e7",
   "metadata": {},
   "source": [
    "## 18. Summary & Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f25cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FRAUD DETECTION MODEL - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Dataset Information:\")\n",
    "print(f\"   Total samples: {len(df):,}\")\n",
    "print(f\"   Features used: {len(available_features)}\")\n",
    "print(f\"   Fraud rate: {y.mean()*100:.4f}%\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model_name}\")\n",
    "best_results = results_df[results_df['Model'] == best_model_name].iloc[0]\n",
    "print(f\"   Accuracy:  {best_results['Accuracy']:.4f}\")\n",
    "print(f\"   Precision: {best_results['Precision']:.4f}\")\n",
    "print(f\"   Recall:    {best_results['Recall']:.4f}\")\n",
    "print(f\"   F1-Score:  {best_results['F1-Score']:.4f}\")\n",
    "print(f\"   ROC-AUC:   {best_results['ROC-AUC']:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Key Findings:\")\n",
    "print(f\"   ‚úì Class imbalance successfully handled with SMOTE\")\n",
    "print(f\"   ‚úì Feature engineering improved model performance\")\n",
    "print(f\"   ‚úì {best_model_name} achieved best balance of precision and recall\")\n",
    "print(f\"   ‚úì Model saved and ready for deployment\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   1. Deploy model using Streamlit app (app.py)\")\n",
    "print(f\"   2. Monitor model performance in production\")\n",
    "print(f\"   3. Collect feedback and retrain periodically\")\n",
    "print(f\"   4. Consider ensemble methods for further improvement\")\n",
    "print(f\"   5. Implement real-time monitoring and alerting\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Analysis Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc9feb",
   "metadata": {},
   "source": [
    "## 19. Load and Test Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a8e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved model\n",
    "loaded_model = joblib.load('fraud_detection_model.pkl')\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Test loaded model\n",
    "test_predictions = loaded_model.predict(X_test[:5])\n",
    "print(f\"\\nTest predictions: {test_predictions}\")\n",
    "print(f\"Actual values: {y_test[:5].values}\")\n",
    "\n",
    "# Verify model performance\n",
    "loaded_pred = loaded_model.predict(X_test)\n",
    "loaded_f1 = f1_score(y_test, loaded_pred)\n",
    "print(f\"\\nLoaded model F1-Score: {loaded_f1:.4f}\")\n",
    "print(\"Model is ready for deployment! üéâ\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
